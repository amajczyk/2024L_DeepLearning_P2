{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 15:14:32.131699: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-22 15:14:32.460301: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-22 15:14:33.241035: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 64721 files belonging to 30 classes.\n",
      "Using 51777 files for training.\n",
      "Using 12944 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 15:14:57.681618: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-22 15:14:57.773516: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-22 15:14:57.773551: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-22 15:14:57.775807: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-22 15:14:57.775838: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-22 15:14:57.775851: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-22 15:14:57.889260: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-22 15:14:57.889299: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-22 15:14:57.889305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-22 15:14:57.889326: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-22 15:14:57.889640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13512 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# import wav data to keras dataset\n",
    "import numpy as np\n",
    "from keras.utils import audio_dataset_from_directory\n",
    "\n",
    "# Load audio dataset from directory\n",
    "train_dataset, validation_dataset = audio_dataset_from_directory(\n",
    "    directory='data/path/train',\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    seed=1337,\n",
    "    batch_size=64,\n",
    "    output_sequence_length=16000, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TFWav2Vec2ForSequenceClassification has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2ForSequenceClassification: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFWav2Vec2ForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFWav2Vec2ForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFWav2Vec2ForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['wav2vec2.masked_spec_embed', 'projector.weight', 'projector.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import AutoFeatureExtractor, TFWav2Vec2ForSequenceClassification\n",
    "\n",
    "# Load pre-trained Wav2Vec2 model and processor using AutoFeatureExtractor\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "tokenizer = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "\n",
    "# Load pre-trained Wav2Vec2 model\n",
    "model = TFWav2Vec2ForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d98d3bd5dc4629b7cf037354840723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/810 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 15:26:32.312595: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157f5322af5240a5a3922f1b914fe70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 15:29:27.108169: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoFeatureExtractor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load Wav2Vec tokenizer\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "tokenizer = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "print(\"test\")\n",
    "\n",
    "# Define a function to tokenize audio data\n",
    "def tokenize_dataset(dataset):\n",
    "    tokenized_batches = []\n",
    "    for batch in tqdm(dataset, total=len(dataset)):\n",
    "        \n",
    "        audio_batch, labels = batch\n",
    "        # print(audio_batch)\n",
    "        inputs = tokenizer(audio_batch, return_tensors=\"tf\", padding=True, verbose=False, sampling_rate=16000)\n",
    "        tokenized_batches.append((inputs.input_values, labels))\n",
    "    return tokenized_batches\n",
    "\n",
    "# Tokenize the train dataset\n",
    "tokenized_train_dataset = tokenize_dataset(train_dataset)\n",
    "\n",
    "# Tokenize the validation dataset\n",
    "tokenized_validation_dataset = tokenize_dataset(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert tokenized batches into a generator function\n",
    "def batch_generator(tokenized_batches):\n",
    "    for input_values, labels in tokenized_batches:\n",
    "        yield input_values, labels\n",
    "\n",
    "# Create Keras batch datasets for train and validation datasets\n",
    "keras_train_dataset = tf.data.Dataset.from_generator(\n",
    "    generator=lambda: batch_generator(tokenized_train_dataset),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(1, 32, 16000, 1), dtype=tf.float32),  # Input values\n",
    "        tf.TensorSpec(shape=(32, 30), dtype=tf.int32)           # Labels\n",
    "    )\n",
    ")\n",
    "\n",
    "keras_validation_dataset = tf.data.Dataset.from_generator(\n",
    "    generator=lambda: batch_generator(tokenized_validation_dataset),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(1, 32, 16000, 1), dtype=tf.float32),  # Input values\n",
    "        tf.TensorSpec(shape=(32, 30), dtype=tf.int32)           # Labels\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.concat([i[0][0] for i in tokenized_train_dataset], axis=0)\n",
    "X_val = tf.concat([i[0][0] for i in tokenized_validation_dataset], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.concat([i[1] for i in tokenized_train_dataset], axis=0)\n",
    "y_val = tf.concat([i[1] for i in tokenized_validation_dataset], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del tokenized_train_dataset\n",
    "del tokenized_validation_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.numpy()\n",
    "X_val = X_val.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Dropout, Dense\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, TimeDistributed\n",
    "\n",
    "\n",
    "def get_SR_Model(num_classes: int):\n",
    "    X_input = Input(shape=(16000, 1))\n",
    "    X = Conv1D(filters=256,kernel_size=15,strides=4)(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Conv1D(filters=512,kernel_size=15,strides=4)(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = LSTM(units=512, return_sequences=True)(X)\n",
    "    X = LSTM(units=512, return_sequences=False)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    return Model(inputs=[X_input], outputs=[X])\n",
    "\n",
    "model = get_SR_Model(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2Model: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFWav2Vec2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFWav2Vec2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFWav2Vec2Model were not initialized from the PyTorch model and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from transformers import TFWav2Vec2Model\n",
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "\n",
    "class TFWav2Vec2Layer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TFWav2Vec2Layer, self).__init__(**kwargs)\n",
    "        self.wav2vec_model = TFWav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.wav2vec_model.trainable = False\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return self.wav2vec_model(inputs)[\"last_hidden_state\"]\n",
    "\n",
    "\n",
    "def get_wav2vec_classifier(num_classes: int):\n",
    "    # Define input layer for tokenized input\n",
    "    tokenized_input = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "    # Pass input through custom Wav2Vec2 layer\n",
    "    hidden_states = TFWav2Vec2Layer()(tokenized_input)\n",
    "\n",
    "    # Add classification layers\n",
    "    x = layers.Dense(512, activation='relu')(hidden_states[:, 0, :])  # Take the first token's representation\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Define model\n",
    "    model = Model(inputs=tokenized_input, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "model = get_wav2vec_classifier(num_classes=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                loss=losses.CategoricalCrossentropy(),\n",
    "                metrics=[metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1713792588.711016    3431 service.cc:145] XLA service 0x7f1ae0010660 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1713792588.711069    3431 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4090 Laptop GPU, Compute Capability 8.9\n",
      "2024-04-22 15:29:49.138951: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "W0000 00:00:1713792589.306986    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.0/attention/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1713792589.722730    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.0/attention/assert_equal_3/Assert/Assert\n",
      "W0000 00:00:1713792589.722934    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1713792589.723012    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.1/attention/assert_equal_3/Assert/Assert\n",
      "W0000 00:00:1713792589.723309    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.2/attention/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1713792589.723382    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.2/attention/assert_equal_3/Assert/Assert\n",
      "W0000 00:00:1713792589.723471    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1713792589.723558    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.3/attention/assert_equal_3/Assert/Assert\n",
      "W0000 00:00:1713792589.723632    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.4/attention/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1713792589.723743    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.4/attention/assert_equal_3/Assert/Assert\n",
      "W0000 00:00:1713792589.723842    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1713792589.723910    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.5/attention/assert_equal_3/Assert/Assert\n",
      "W0000 00:00:1713792589.724019    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.6/attention/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1713792589.724131    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.6/attention/assert_equal_3/Assert/Assert\n",
      "W0000 00:00:1713792589.724243    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.7/attention/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1713792589.724330    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.7/attention/assert_equal_3/Assert/Assert\n",
      "W0000 00:00:1713792589.724405    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1713792589.724489    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.8/attention/assert_equal_3/Assert/Assert\n",
      "W0000 00:00:1713792589.724631    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.9/attention/assert_equal_1/Assert/Assert\n",
      "W0000 00:00:1713792589.724757    3431 assert_op.cc:38] Ignoring Assert operator functional_3_1/tf_wav2_vec2_layer_1/tf_wav2_vec2_model/wav2vec2/encoder/layers.9/attention/assert_equal_3/Assert/Assert\n",
      "2024-04-22 15:29:50.319531: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8900\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1713792592.436816    9904 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10716', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1713792593.271349    9891 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_1876', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  3/810\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 59ms/step - categorical_accuracy: 0.0382 - loss: 3.4199 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1713792599.783438    3431 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m809/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - categorical_accuracy: 0.0872 - loss: 3.2731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1713792651.554661   10425 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_6', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - categorical_accuracy: 0.0872 - loss: 3.2730"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1713792672.287529   10741 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_539', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 95ms/step - categorical_accuracy: 0.0873 - loss: 3.2728 - val_categorical_accuracy: 0.2059 - val_loss: 2.8883\n",
      "Epoch 2/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 74ms/step - categorical_accuracy: 0.2005 - loss: 2.8546 - val_categorical_accuracy: 0.2365 - val_loss: 2.7212\n",
      "Epoch 3/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 74ms/step - categorical_accuracy: 0.2356 - loss: 2.6971 - val_categorical_accuracy: 0.2614 - val_loss: 2.6233\n",
      "Epoch 4/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 73ms/step - categorical_accuracy: 0.2578 - loss: 2.6071 - val_categorical_accuracy: 0.2834 - val_loss: 2.5263\n",
      "Epoch 5/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 73ms/step - categorical_accuracy: 0.2820 - loss: 2.5339 - val_categorical_accuracy: 0.2841 - val_loss: 2.5250\n",
      "Epoch 6/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 74ms/step - categorical_accuracy: 0.2775 - loss: 2.5336 - val_categorical_accuracy: 0.2982 - val_loss: 2.4648\n",
      "Epoch 7/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 74ms/step - categorical_accuracy: 0.2944 - loss: 2.4722 - val_categorical_accuracy: 0.3078 - val_loss: 2.4346\n",
      "Epoch 8/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 84ms/step - categorical_accuracy: 0.3006 - loss: 2.4488 - val_categorical_accuracy: 0.3036 - val_loss: 2.4334\n",
      "Epoch 9/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 76ms/step - categorical_accuracy: 0.3047 - loss: 2.4287 - val_categorical_accuracy: 0.3083 - val_loss: 2.4287\n",
      "Epoch 10/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 76ms/step - categorical_accuracy: 0.3150 - loss: 2.3939 - val_categorical_accuracy: 0.3192 - val_loss: 2.3772\n",
      "Epoch 11/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3182 - loss: 2.3730 - val_categorical_accuracy: 0.3256 - val_loss: 2.3651\n",
      "Epoch 12/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 75ms/step - categorical_accuracy: 0.3250 - loss: 2.3459 - val_categorical_accuracy: 0.3281 - val_loss: 2.3570\n",
      "Epoch 13/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 75ms/step - categorical_accuracy: 0.3324 - loss: 2.3218 - val_categorical_accuracy: 0.3254 - val_loss: 2.3508\n",
      "Epoch 14/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3333 - loss: 2.3134 - val_categorical_accuracy: 0.3364 - val_loss: 2.3264\n",
      "Epoch 15/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3366 - loss: 2.3100 - val_categorical_accuracy: 0.3183 - val_loss: 2.3879\n",
      "Epoch 16/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3352 - loss: 2.3066 - val_categorical_accuracy: 0.3360 - val_loss: 2.3226\n",
      "Epoch 17/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 75ms/step - categorical_accuracy: 0.3460 - loss: 2.2705 - val_categorical_accuracy: 0.3404 - val_loss: 2.3034\n",
      "Epoch 18/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3456 - loss: 2.2591 - val_categorical_accuracy: 0.3457 - val_loss: 2.2975\n",
      "Epoch 19/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3484 - loss: 2.2480 - val_categorical_accuracy: 0.3463 - val_loss: 2.2798\n",
      "Epoch 20/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3549 - loss: 2.2399 - val_categorical_accuracy: 0.3482 - val_loss: 2.2897\n",
      "Epoch 21/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3560 - loss: 2.2307 - val_categorical_accuracy: 0.3432 - val_loss: 2.2973\n",
      "Epoch 22/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3569 - loss: 2.2183 - val_categorical_accuracy: 0.3474 - val_loss: 2.2887\n",
      "Epoch 23/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3593 - loss: 2.2099 - val_categorical_accuracy: 0.3505 - val_loss: 2.2722\n",
      "Epoch 24/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3612 - loss: 2.1972 - val_categorical_accuracy: 0.3447 - val_loss: 2.3119\n",
      "Epoch 25/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 76ms/step - categorical_accuracy: 0.3459 - loss: 2.2424 - val_categorical_accuracy: 0.3460 - val_loss: 2.2869\n",
      "Epoch 26/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 76ms/step - categorical_accuracy: 0.3597 - loss: 2.2013 - val_categorical_accuracy: 0.3529 - val_loss: 2.2814\n",
      "Epoch 27/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3604 - loss: 2.2004 - val_categorical_accuracy: 0.3522 - val_loss: 2.2651\n",
      "Epoch 28/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3682 - loss: 2.1732 - val_categorical_accuracy: 0.3416 - val_loss: 2.3212\n",
      "Epoch 29/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3619 - loss: 2.2030 - val_categorical_accuracy: 0.3531 - val_loss: 2.2797\n",
      "Epoch 30/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 76ms/step - categorical_accuracy: 0.3598 - loss: 2.2063 - val_categorical_accuracy: 0.3586 - val_loss: 2.2641\n",
      "Epoch 31/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 76ms/step - categorical_accuracy: 0.3705 - loss: 2.1697 - val_categorical_accuracy: 0.3506 - val_loss: 2.2980\n",
      "Epoch 32/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3702 - loss: 2.1632 - val_categorical_accuracy: 0.3495 - val_loss: 2.2965\n",
      "Epoch 33/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3717 - loss: 2.1549 - val_categorical_accuracy: 0.3544 - val_loss: 2.2798\n",
      "Epoch 34/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3730 - loss: 2.1592 - val_categorical_accuracy: 0.3594 - val_loss: 2.2614\n",
      "Epoch 35/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3684 - loss: 2.1662 - val_categorical_accuracy: 0.3528 - val_loss: 2.2763\n",
      "Epoch 36/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3744 - loss: 2.1440 - val_categorical_accuracy: 0.3511 - val_loss: 2.2859\n",
      "Epoch 37/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3821 - loss: 2.1137 - val_categorical_accuracy: 0.3505 - val_loss: 2.3006\n",
      "Epoch 38/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 76ms/step - categorical_accuracy: 0.3692 - loss: 2.1542 - val_categorical_accuracy: 0.3576 - val_loss: 2.2711\n",
      "Epoch 39/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3820 - loss: 2.1215 - val_categorical_accuracy: 0.3590 - val_loss: 2.2715\n",
      "Epoch 40/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 76ms/step - categorical_accuracy: 0.3721 - loss: 2.1509 - val_categorical_accuracy: 0.3559 - val_loss: 2.2916\n",
      "Epoch 41/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 75ms/step - categorical_accuracy: 0.3860 - loss: 2.1003 - val_categorical_accuracy: 0.3553 - val_loss: 2.2850\n",
      "Epoch 42/100\n",
      "\u001b[1m810/810\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 75ms/step - categorical_accuracy: 0.3872 - loss: 2.1061 - val_categorical_accuracy: 0.3592 - val_loss: 2.2790\n",
      "Epoch 43/100\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
