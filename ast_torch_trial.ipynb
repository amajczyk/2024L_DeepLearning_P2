{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "class CustomAudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None, fixed_length=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.file_list, self.labels = self._get_file_list_and_labels()\n",
    "        self.transform = transform\n",
    "        self.fixed_length = fixed_length\n",
    "\n",
    "    def _get_file_list_and_labels(self):\n",
    "        file_list = []\n",
    "        labels = []\n",
    "        for root, dirs, files in os.walk(self.data_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):  # Adjust file extension if needed\n",
    "                    file_list.append(os.path.join(root, file))\n",
    "                    labels.append(os.path.basename(root))  # Extract label from directory name\n",
    "        return file_list, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        \n",
    "        if self.fixed_length:\n",
    "            waveform = self._pad_waveform(waveform, self.fixed_length)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return waveform, sample_rate, label\n",
    "\n",
    "    def _pad_waveform(self, waveform, target_length):\n",
    "        length_diff = target_length - waveform.size(1)\n",
    "        if length_diff > 0:\n",
    "            padding = torch.zeros((1, length_diff))\n",
    "            waveform = torch.cat([waveform, padding], dim=1)\n",
    "        return waveform\n",
    "\n",
    "# Example usage\n",
    "data_dir = \"data/path/train/\"\n",
    "transform = None  # You can define transformations if needed\n",
    "fixed_length = 16000  # Assuming you want to fix the length to 16000 samples\n",
    "\n",
    "# Create custom dataset\n",
    "dataset = CustomAudioDataset(data_dir, transform=transform, fixed_length=fixed_length)\n",
    "\n",
    "# Create a DataLoader to iterate over the dataset\n",
    "batch_size = 32\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('wow', 'yes', 'off', 'up', 'right', 'happy', 'nine', 'wow', 'seven', 'six', 'two', 'cat', 'marvin', 'down', 'up', 'tree', 'nine', 'dog', 'bed', 'down', 'happy', 'six', 'house', 'on', 'go', 'down', 'one', 'right', 'eight', 'bird', 'bird', 'on')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('six', 'down', 'bed', 'left', 'two', 'happy', 'three', 'left', 'two', 'nine', 'right', 'bird', 'house', 'tree', 'no', 'left', 'on', 'stop', 'nine', 'dog', 'tree', 'go', 'tree', 'marvin', 'bed', 'bed', 'zero', 'zero', 'five', 'go', 'wow', 'dog')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('house', 'down', 'yes', 'five', 'five', 'right', 'off', 'left', 'one', 'five', 'marvin', 'go', 'stop', 'bird', 'marvin', 'six', 'four', 'nine', 'yes', 'house', 'sheila', 'zero', 'five', 'left', 'one', 'sheila', 'happy', 'sheila', 'left', 'sheila', 'five', 'stop')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('tree', 'marvin', 'house', 'cat', 'two', 'one', 'bed', 'dog', 'happy', 'house', 'eight', 'house', 'left', 'no', 'nine', 'three', 'eight', 'eight', 'marvin', 'seven', 'three', 'two', 'dog', 'right', 'go', 'stop', 'off', 'no', 'dog', 'three', 'yes', 'go')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('tree', 'left', 'bird', 'two', 'five', 'zero', 'eight', 'wow', 'zero', 'up', 'off', 'three', 'up', 'house', 'up', 'one', 'no', 'bed', 'left', 'on', 'house', 'on', 'marvin', 'two', 'wow', 'dog', 'off', 'bed', 'left', 'right', 'four', 'yes')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('two', 'bird', 'zero', 'happy', 'yes', 'wow', 'four', 'tree', 'no', 'yes', 'one', 'nine', 'zero', 'house', 'wow', 'six', 'on', 'one', 'house', 'two', 'dog', 'right', 'eight', 'dog', 'down', 'five', 'six', 'four', 'six', 'eight', 'off', 'yes')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('seven', 'dog', 'sheila', 'one', 'bird', 'down', 'down', 'four', 'no', 'cat', 'one', 'left', 'down', 'tree', 'down', 'five', 'bed', 'eight', 'nine', 'marvin', 'left', 'two', 'on', 'four', 'four', 'five', 'no', 'nine', 'go', 'six', 'up', 'three')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('sheila', 'yes', 'up', 'on', 'one', 'five', 'wow', 'left', 'tree', 'stop', 'right', 'two', 'stop', 'down', 'nine', 'left', 'yes', 'off', 'dog', 'down', 'nine', 'no', 'three', 'marvin', 'nine', 'three', 'one', 'eight', 'no', 'bed', 'six', 'nine')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('six', 'house', 'house', 'zero', 'dog', 'on', 'three', 'go', 'yes', 'right', 'down', 'down', 'sheila', 'five', 'one', 'wow', 'nine', 'yes', 'bed', 'down', 'on', 'stop', 'dog', 'on', 'seven', 'one', 'five', 'five', 'house', 'on', 'on', 'down')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('right', 'happy', 'yes', 'five', 'three', 'nine', 'nine', 'six', 'no', 'right', 'nine', 'four', 'right', 'go', 'bed', 'eight', 'house', 'bed', 'down', 'down', 'three', 'on', 'six', 'down', 'two', 'two', 'bird', 'left', 'yes', 'house', 'go', 'two')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('stop', 'dog', 'dog', 'tree', 'happy', 'dog', 'nine', 'two', 'seven', 'left', 'wow', 'two', 'go', 'five', 'two', 'nine', 'up', 'off', 'stop', 'stop', 'happy', 'left', 'nine', 'happy', 'four', 'off', 'nine', 'seven', 'house', 'nine', 'off', 'sheila')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('seven', 'zero', 'seven', 'go', 'six', 'zero', 'left', 'five', 'eight', 'house', 'left', 'no', 'nine', 'dog', 'left', 'happy', 'seven', 'no', 'yes', 'on', 'three', 'left', 'stop', 'zero', 'up', 'one', 'eight', 'nine', 'dog', 'three', 'left', 'four')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('stop', 'zero', 'happy', 'eight', 'off', 'left', 'six', 'seven', 'nine', 'tree', 'happy', 'left', 'bed', 'two', 'tree', 'no', 'four', 'bed', 'cat', 'marvin', 'six', 'right', 'nine', 'on', 'sheila', 'stop', 'go', 'six', 'seven', 'stop', 'seven', 'wow')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('marvin', 'down', 'no', 'nine', 'stop', 'wow', 'bed', 'dog', 'seven', 'on', 'go', 'marvin', 'tree', 'zero', 'cat', 'seven', 'sheila', 'go', 'zero', 'nine', 'eight', 'eight', 'nine', 'three', 'five', 'no', 'seven', 'off', 'cat', 'house', 'bed', 'wow')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('up', 'house', 'seven', 'down', 'down', 'go', 'wow', 'bird', 'on', 'on', 'tree', 'on', 'left', 'go', 'one', 'on', 'four', 'eight', 'wow', 'one', 'five', 'eight', 'cat', 'two', 'off', 'yes', 'two', 'go', 'go', 'zero', 'left', 'six')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('on', 'down', 'right', 'seven', 'two', 'yes', 'marvin', 'happy', 'off', 'cat', 'up', 'seven', 'sheila', 'marvin', 'yes', 'nine', 'seven', 'left', 'off', 'eight', 'marvin', 'zero', 'three', 'seven', 'house', 'two', 'yes', 'six', 'eight', 'seven', 'five', 'left')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('stop', 'seven', 'dog', 'stop', 'five', 'off', 'marvin', 'left', 'go', 'six', 'four', 'nine', 'zero', 'go', 'down', 'eight', 'bed', 'stop', 'two', 'sheila', 'bird', 'eight', 'tree', 'up', 'bird', 'off', 'marvin', 'tree', 'bed', 'three', 'down', 'left')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('happy', 'dog', 'two', 'one', 'four', 'four', 'tree', 'nine', 'no', 'four', 'house', 'off', 'tree', 'down', 'wow', 'nine', 'right', 'five', 'two', 'five', 'dog', 'dog', 'go', 'eight', 'yes', 'left', 'house', 'right', 'cat', 'three', 'four', 'eight')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('on', 'happy', 'no', 'three', 'on', 'yes', 'left', 'happy', 'sheila', 'down', 'yes', 'tree', 'bed', 'bird', 'bed', 'zero', 'cat', 'off', 'stop', 'one', 'three', 'yes', 'five', 'tree', 'two', 'no', 'yes', 'bird', 'seven', 'seven', 'marvin', 'dog')\n",
      "torch.Size([32, 1, 16000]) tensor([16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000, 16000,\n",
      "        16000, 16000]) ('up', 'down', 'nine', 'on', 'four', 'nine', 'cat', 'up', 'on', 'six', 'sheila', 'house', 'bed', 'one', 'zero', 'sheila', 'left', 'nine', 'seven', 'zero', 'three', 'wow', 'marvin', 'no', 'two', 'zero', 'no', 'house', 'go', 'bed', 'nine', 'one')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Now you can iterate over data_loader to get batches of audio waveforms, sample rates, and labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m waveforms, sample_rates, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Your training/validation loop goes here\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(waveforms\u001b[38;5;241m.\u001b[39msize(), sample_rates, labels)\n",
      "File \u001b[0;32m/mnt/c/Users/adamm/Documents/PW/Sem8/Deep Learning/2024L_DeepLearning_P2/.venvTorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/mnt/c/Users/adamm/Documents/PW/Sem8/Deep Learning/2024L_DeepLearning_P2/.venvTorch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/c/Users/adamm/Documents/PW/Sem8/Deep Learning/2024L_DeepLearning_P2/.venvTorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/c/Users/adamm/Documents/PW/Sem8/Deep Learning/2024L_DeepLearning_P2/.venvTorch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36mCustomAudioDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     27\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list[idx]\n\u001b[0;32m---> 28\u001b[0m     waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     31\u001b[0m         waveform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(waveform)\n",
      "File \u001b[0;32m/mnt/c/Users/adamm/Documents/PW/Sem8/Deep Learning/2024L_DeepLearning_P2/.venvTorch/lib/python3.10/site-packages/torchaudio/_backend/utils.py:205\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/adamm/Documents/PW/Sem8/Deep Learning/2024L_DeepLearning_P2/.venvTorch/lib/python3.10/site-packages/torchaudio/_backend/soundfile.py:27\u001b[0m, in \u001b[0;36mSoundfileBackend.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     19\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[1;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msoundfile_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/adamm/Documents/PW/Sem8/Deep Learning/2024L_DeepLearning_P2/.venvTorch/lib/python3.10/site-packages/torchaudio/_backend/soundfile_backend.py:221\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;129m@_requires_soundfile\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    141\u001b[0m     filepath: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28mformat\u001b[39m: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from file.\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    Note:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msoundfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file_:\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m file_\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWAV\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m normalize:\n\u001b[1;32m    223\u001b[0m             dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/c/Users/adamm/Documents/PW/Sem8/Deep Learning/2024L_DeepLearning_P2/.venvTorch/lib/python3.10/site-packages/soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/mnt/c/Users/adamm/Documents/PW/Sem8/Deep Learning/2024L_DeepLearning_P2/.venvTorch/lib/python3.10/site-packages/soundfile.py:1205\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1204\u001b[0m             file \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mencode(_sys\u001b[38;5;241m.\u001b[39mgetfilesystemencoding())\n\u001b[0;32m-> 1205\u001b[0m     file_ptr \u001b[38;5;241m=\u001b[39m \u001b[43mopenfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m   1207\u001b[0m     file_ptr \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_open_fd(file, mode_int, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info, closefd)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Now you can iterate over data_loader to get batches of audio waveforms, sample rates, and labels\n",
    "for waveforms, sample_rates, labels in data_loader:\n",
    "    # Your training/validation loop goes here\n",
    "    print(waveforms.size(), sample_rates, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import TFFlaxWhisperForAudioClassification, UnivNetModel\n",
    "\n",
    "# Load pre-trained Wav2Vec2 model and processor using AutoFeatureExtractor\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "\n",
    "# Load pre-trained Wav2Vec2 model\n",
    "model = UnivNetFeatureExtractor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoFeatureExtractor\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load Wav2Vec tokenizer\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "tokenizer = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "print(\"test\")\n",
    "\n",
    "# Define a function to tokenize audio data\n",
    "def tokenize_dataset(dataset):\n",
    "    tokenized_batches = []\n",
    "    for batch in tqdm(dataset, total=len(dataset)):\n",
    "        \n",
    "        audio_batch, labels = batch\n",
    "        # print(audio_batch)\n",
    "        inputs = tokenizer(audio_batch, return_tensors=\"tf\", padding=True, verbose=False, sampling_rate=16000)\n",
    "        tokenized_batches.append((inputs.input_values, labels))\n",
    "    return tokenized_batches\n",
    "\n",
    "# Tokenize the train dataset\n",
    "tokenized_train_dataset = tokenize_dataset(train_dataset)\n",
    "\n",
    "# Tokenize the validation dataset\n",
    "tokenized_validation_dataset = tokenize_dataset(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert tokenized batches into a generator function\n",
    "def batch_generator(tokenized_batches):\n",
    "    for input_values, labels in tokenized_batches:\n",
    "        yield input_values, labels\n",
    "\n",
    "# Create Keras batch datasets for train and validation datasets\n",
    "keras_train_dataset = tf.data.Dataset.from_generator(\n",
    "    generator=lambda: batch_generator(tokenized_train_dataset),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(1, 32, 16000, 1), dtype=tf.float32),  # Input values\n",
    "        tf.TensorSpec(shape=(32, 30), dtype=tf.int32)           # Labels\n",
    "    )\n",
    ")\n",
    "\n",
    "keras_validation_dataset = tf.data.Dataset.from_generator(\n",
    "    generator=lambda: batch_generator(tokenized_validation_dataset),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(1, 32, 16000, 1), dtype=tf.float32),  # Input values\n",
    "        tf.TensorSpec(shape=(32, 30), dtype=tf.int32)           # Labels\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.concat([i[0][0] for i in tokenized_train_dataset], axis=0)\n",
    "X_val = tf.concat([i[0][0] for i in tokenized_validation_dataset], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.concat([i[1] for i in tokenized_train_dataset], axis=0)\n",
    "y_val = tf.concat([i[1] for i in tokenized_validation_dataset], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del tokenized_train_dataset\n",
    "del tokenized_validation_dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.numpy()\n",
    "X_val = X_val.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Dropout, Dense\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, TimeDistributed\n",
    "\n",
    "\n",
    "def get_SR_Model(num_classes: int):\n",
    "    X_input = Input(shape=(16000, 1))\n",
    "    X = Conv1D(filters=256,kernel_size=15,strides=4)(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = Conv1D(filters=512,kernel_size=15,strides=4)(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.2)(X)\n",
    "    X = LSTM(units=512, return_sequences=True)(X)\n",
    "    X = LSTM(units=512, return_sequences=False)(X)\n",
    "    X = Dense(num_classes, activation='softmax')(X)\n",
    "    return Model(inputs=[X_input], outputs=[X])\n",
    "\n",
    "model = get_SR_Model(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from transformers import TFWav2Vec2Model\n",
    "from tensorflow.keras import Model, layers\n",
    "\n",
    "\n",
    "class TFWav2Vec2Layer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TFWav2Vec2Layer, self).__init__(**kwargs)\n",
    "        self.wav2vec_model = TFWav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.wav2vec_model.trainable = False\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return self.wav2vec_model(inputs)[\"last_hidden_state\"]\n",
    "\n",
    "\n",
    "def get_wav2vec_classifier(num_classes: int):\n",
    "    # Define input layer for tokenized input\n",
    "    tokenized_input = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "\n",
    "    # Pass input through custom Wav2Vec2 layer\n",
    "    hidden_states = TFWav2Vec2Layer()(tokenized_input)\n",
    "\n",
    "    # Add classification layers\n",
    "    x = layers.Dense(512, activation='relu')(hidden_states[:, 0, :])  # Take the first token's representation\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Define model\n",
    "    model = Model(inputs=tokenized_input, outputs=x)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "model = get_wav2vec_classifier(num_classes=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                loss=losses.CategoricalCrossentropy(),\n",
    "                metrics=[metrics.CategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
